<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>【若干大作业】RNN三连 | Four&#39;s</title><meta name="description" content="&amp;emsp;&amp;emsp;这学期一口气选了三门 AI 课（AI、模式识别、NLP），初衷就是想深入了解以后能更有底气地说“我不喜欢AI”（x&amp;emsp;&amp;emsp;然后三门课内容高度重复，每个知识点平均听三遍。。。其中最近发生的重合是，人工智能实验先要写一个 RNN 做关键词提取，然后 NLP 课要用 BiLSTM+CRF 做中文分词，完了之后还要用 LSTM 做语言模型。。。&amp;emsp;&amp;emsp"><meta property="og:type" content="article"><meta property="og:title" content="【若干大作业】RNN三连"><meta property="og:url" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/index.html"><meta property="og:site_name" content="Four&#39;s"><meta property="og:description" content="&amp;emsp;&amp;emsp;这学期一口气选了三门 AI 课（AI、模式识别、NLP），初衷就是想深入了解以后能更有底气地说“我不喜欢AI”（x&amp;emsp;&amp;emsp;然后三门课内容高度重复，每个知识点平均听三遍。。。其中最近发生的重合是，人工智能实验先要写一个 RNN 做关键词提取，然后 NLP 课要用 BiLSTM+CRF 做中文分词，完了之后还要用 LSTM 做语言模型。。。&amp;emsp;&amp;emsp"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn1.jpg"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn2.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/lstm1.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/lstm2.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn3.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn4.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn5.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn6.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn7.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn8.png"><meta property="og:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn9.png"><meta property="article:published_time" content="2020-12-02T05:11:04.000Z"><meta property="article:modified_time" content="2022-05-12T11:52:40.000Z"><meta property="article:author" content="kqp"><meta property="article:tag" content="RNN"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn1.jpg"><link rel="canonical" href="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/index.html"><link rel="alternate" href="/atom.xml" title="Four&#39;s" type="application/atom+xml"><link rel="icon" href="/favicon_you_3.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet"><meta name="generator" content="Hexo 6.2.0"></head><body class="main-center" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.png" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">kqp</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">OIACMer / HKU_CS / LLer / TCS</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i></small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="Search"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">Home</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">Archives</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">Categories</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">Tags</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">Links</span></a></li><li class="menu-item menu-item-about"><a href="/personal/index"><i class="icon icon-cup-fill"></i> <span class="menu-title">About</span></a></li></ul><ul class="social-links"><li><a href="mailto:kuangqp1217@qq.com" target="_blank" title="Email" data-toggle="tooltip" data-placement="top"><i class="icon icon-email"></i></a></li><li><a href="http://wpa.qq.com/msgrd?v=3&uin=940240973&site=qq&menu=yes" target="_blank" title="Qq" data-toggle="tooltip" data-placement="top"><i class="icon icon-qq"></i></a></li><li><a href="https://twitter.com/yuan_four" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="https://github.com/l2l7l9p" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">Board</h3><div class="widget-body"><div id="board"><div class="content"><p>总有一天会学习前端，亲自操刀改造博客的。（发出鸽子的声音）</p></div></div></div></div><div class="widget"><h3 class="widget-title">Categories</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/OI-ACM/">OI/ACM</a><span class="category-list-count">199</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/TCS/">TCS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%80%BB%E7%BB%93%E4%B8%8E%E6%B8%B8%E8%AE%B0/">总结与游记</a><span class="category-list-count">36</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E5%86%99/">杂写</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%8E%A9/">玩</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E6%89%8Bacademy/">随手academy</a><span class="category-list-count">2</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Tag Cloud</h3><div class="widget-body tagcloud"><a href="/tags/CPU/" style="font-size:13px">CPU</a> <a href="/tags/RNN/" style="font-size:13px">RNN</a> <a href="/tags/ZKP/" style="font-size:13px">ZKP</a> <a href="/tags/complexity/" style="font-size:13px">complexity</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size:13px">操作系统</a> <a href="/tags/%E7%A8%8B%E8%AE%BE/" style="font-size:13.08px">程设</a> <a href="/tags/%E7%AE%97%E6%B3%95-2-SAT/" style="font-size:13.08px">算法_2-SAT</a> <a href="/tags/%E7%AE%97%E6%B3%95-DP/" style="font-size:13.92px">算法_DP</a> <a href="/tags/%E7%AE%97%E6%B3%95-FFT-NTT/" style="font-size:13.46px">算法_FFT/NTT</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E4%BD%8D%E8%BF%90%E7%AE%97/" style="font-size:13.69px">算法_位运算</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%81%8F%E5%BA%8F%E5%85%B3%E7%B3%BB/" style="font-size:13.08px">算法_偏序关系</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%87%A0%E4%BD%95/" style="font-size:13.08px">算法_几何</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%88%86%E6%B2%BB/" style="font-size:13.15px">算法_分治</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%8D%9A%E5%BC%88/" style="font-size:13.08px">算法_博弈</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%90%AF%E5%8F%91%E5%BC%8F%E5%90%88%E5%B9%B6/" style="font-size:13.08px">算法_启发式合并</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%9B%BE%E8%AE%BA/" style="font-size:13.54px">算法_图论</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%A4%9A%E9%A1%B9%E5%BC%8F-%E7%94%9F%E6%88%90%E5%87%BD%E6%95%B0/" style="font-size:13.54px">算法_多项式/生成函数</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size:13.38px">算法_字符串</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%B9%B3%E8%A1%A1%E6%A0%91-set/" style="font-size:13.08px">算法_平衡树/set</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%B9%B3%E8%A1%A1%E8%A7%84%E5%88%92%EF%BC%88%E5%AE%9A%E6%9C%9F%E9%87%8D%E6%9E%84%EF%BC%89/" style="font-size:13px">算法_平衡规划（定期重构）</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size:13.31px">算法_并查集</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%89%AB%E6%8F%8F%E7%BA%BF/" style="font-size:13.08px">算法_扫描线</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%95%B0%E5%AD%A6/" style="font-size:13.08px">算法_数学</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%95%B0%E8%AE%BA/" style="font-size:13.85px">算法_数论</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%9C%80%E7%9F%AD%E8%B7%AF%E6%A8%A1%E5%9E%8B/" style="font-size:13.23px">算法_最短路模型</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%9E%84%E9%80%A0%E9%A2%98/" style="font-size:13.62px">算法_构造题</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%9E%90%E5%90%88%E6%A0%91/" style="font-size:13px">算法_析合树</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%A0%91%E9%93%BE%E5%89%96%E5%88%86/" style="font-size:13.08px">算法_树链剖分</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%A0%B9%E5%8F%B7%E5%B9%B3%E8%A1%A1/" style="font-size:13.23px">算法_根号平衡</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E6%A6%82%E7%8E%87%E4%B8%8E%E6%9C%9F%E6%9C%9B/" style="font-size:13.23px">算法_概率与期望</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%82%B9%E5%88%86%E6%B2%BB/" style="font-size:13.15px">算法_点分治</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%8E%AF%E5%A5%97%E6%A0%91/" style="font-size:13.08px">算法_环套树</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/" style="font-size:13.31px">算法_矩阵乘法</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%A5%9E%E5%A5%87%E7%9A%84%E8%84%91%E6%B4%9E/" style="font-size:14px">算法_神奇的脑洞</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size:13.23px">算法_线性代数</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%BA%BF%E6%AE%B5%E6%A0%91/" style="font-size:13.77px">算法_线段树</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%BD%91%E7%BB%9C%E6%B5%81-%E5%8C%B9%E9%85%8D/" style="font-size:13.54px">算法_网络流/匹配</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E7%BE%A4%E8%AE%BA/" style="font-size:13.08px">算法_群论</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E8%8E%AB%E9%98%9F-%E5%88%86%E5%9D%97/" style="font-size:13.46px">算法_莫队/分块</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E8%AE%A1%E6%95%B0/" style="font-size:13.08px">算法_计数</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E8%B4%AA%E5%BF%83/" style="font-size:13.23px">算法_贪心</a> <a href="/tags/%E7%AE%97%E6%B3%95-%E9%9A%8F%E6%9C%BA%E5%A4%A7%E6%B3%95/" style="font-size:13.38px">算法_随机大法</a> <a href="/tags/%E7%BC%96%E8%AF%91%E5%99%A8/" style="font-size:13.08px">编译器</a></div></div><div class="widget"><h3 class="widget-title">Archive</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">Recent Posts</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E7%8E%A9/">玩</a></p><p class="item-title"><a href="/%E8%99%B9%E5%92%B25th%20live/" class="title">虹咲5th live</a></p><p class="item-date"><time datetime="2022-09-10T13:43:58.000Z" itemprop="datePublished">2022-09-10</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/%E7%8E%A9/">玩</a></p><p class="item-title"><a href="/Liella_S2/" class="title">Liella 第二季随笔</a></p><p class="item-date"><time datetime="2022-08-01T15:21:06.000Z" itemprop="datePublished">2022-08-01</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/OI-ACM/">OI/ACM</a><i class="icon icon-angle-right"></i><a class="category-link" href="/categories/%E6%80%BB%E7%BB%93%E4%B8%8E%E6%B8%B8%E8%AE%B0/">总结与游记</a></p><p class="item-title"><a href="/2021%20EC%20Final%20%E9%80%80%E4%BC%91%E6%B8%B8%E8%AE%B0/" class="title">2021 EC Final 退休游记</a></p><p class="item-date"><time datetime="2022-07-22T06:37:00.000Z" itemprop="datePublished">2022-07-22</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/OI-ACM/">OI/ACM</a></p><p class="item-title"><a href="/%E3%80%90AtCoder%20Grand%20029F%E3%80%91Construction%20of%20a%20tree%20%E9%A2%98%E8%A7%A3/" class="title">【AtCoder Grand 029F】Construction of a tree 题解</a></p><p class="item-date"><time datetime="2022-06-17T06:58:32.000Z" itemprop="datePublished">2022-06-17</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/OI-ACM/">OI/ACM</a></p><p class="item-title"><a href="/%E3%80%90AtCoder%20Regular%20141D%E3%80%91Non-divisible%20Set%20%E9%A2%98%E8%A7%A3/" class="title">【AtCoder Regular 141D】Non-divisible Set 题解</a></p><p class="item-date"><time datetime="2022-05-31T08:45:50.000Z" itemprop="datePublished">2022-05-31</time></p></div></li></ul></div></div></div></aside><main class="main" role="main"><div class="content"><article id="post-【若干大作业】RNN三连" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">【若干大作业】RNN三连</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/" class="article-date"><time datetime="2020-12-02T05:11:04.000Z" itemprop="datePublished">2020-12-02</time> </a></span><span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/project/">project</a> </span><span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link-link" href="/tags/RNN/" rel="tag">RNN</a> </span><span class="article-read hidden-xs"><i class="icon icon-eye-fill" aria-hidden="true"></i> <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span> </span></span><span class="post-comment"><i class="icon icon-comment"></i> <a href="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/#comments" class="article-comment-link">Comments</a></span></div></div><div class="article-entry marked-body" itemprop="articleBody"><p>&emsp;&emsp;这学期一口气选了三门 AI 课（AI、模式识别、NLP），初衷就是想深入了解以后能更有底气地说“我不喜欢AI”（x<br>&emsp;&emsp;然后三门课内容高度重复，每个知识点平均听三遍。。。其中最近发生的重合是，人工智能实验先要写一个 RNN 做关键词提取，然后 NLP 课要用 BiLSTM+CRF 做中文分词，完了之后还要用 LSTM 做语言模型。。。<br>&emsp;&emsp;于是这位可怜的老 C++ 选手在用 C++ 写完了 KNN、决策树、PLA、逻辑回归、BPNN 之后，不得不在一个月内从 python 语法入门摸爬打滚到机器学习带师（x</p><p>&emsp;&emsp;这篇博客大概只是分享和记录，不是教程。<strong>我认为学 AI 最好的方式是在学校里上课（有老师带，有同学一起讨论），或者买本书来学。在网上找博客自学是很不靠谱的。</strong></p><span id="more"></span><h1 id="前置"><a href="#前置" class="headerlink" title="前置"></a>前置</h1><p>&emsp;&emsp;做这些事的前置技能大概就是：学会 python、学会神经网络的理论框架。</p><p>&emsp;&emsp;一开始作为 C++ 选手也本能地纠结了一下既然 pytorch 也有 C++ API，为啥还要转 python 呢？大概考虑三个因素：</p><ol><li>从速度来说：大家心中都有一个观念是“C++跑得快”，但是写起神经网络来，基本上都是在调库，时间瓶颈在于训练和反向传播的那些矩阵运算，线程优化和 GPU 算力才是硬道理。pytorch 底层也是 C/C++ 实现的，所以通篇都在调库的情况下，没有多大时间差。</li><li>从操作方便性来说：编程简便性是 python 优胜毫无疑问了，python 对于数据处理有更灵活的语法，还有巨多方便的库，例如一行 one-hot，例如许多课程可能要求先手写 BPNN，在 numpy 的支撑下 python 会从时间和简便性两方面吊打 C++。python 有包管理器这一点也很资瓷。</li><li>从环境来说：你周围的同学应该 98% 都会跟风用 python，这意味着如果你执意用别的语言，你将基本单打独斗，你们的讨论将变得困难，<del>你代码出了 bug 不会有人来帮你调</del>。这点对于学生可能才是最应该考虑的，为了获得学习环境的兼容，一起跟风吧~</li></ol><p>&emsp;&emsp;然后神经网络的数学基础，大概就是《最优化方法》一类的课程，最好手写一遍 BPNN，该造的轮子必须得造。</p><h1 id="RNN-关键词提取"><a href="#RNN-关键词提取" class="headerlink" title="RNN 关键词提取"></a>RNN 关键词提取</h1><h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>&emsp;&emsp;数据集是 <a target="_blank" rel="noopener" href="http://alt.qcri.org/semeval2014/task4/">http://alt.qcri.org/semeval2014/task4/</a> 的 SemEval-2014,Laptop，里面有 3000 余条英文句子，给出了每个句子的关键词（或词组）。每个句子可能有多个关键词（或词组）。<br>&emsp;&emsp;以此训练一个模型，输入一个英文句子，可以找出句子的关键词。</p><h2 id="模型准备"><a href="#模型准备" class="headerlink" title="模型准备"></a>模型准备</h2><p>&emsp;&emsp;首先要将这个任务表示为具体的数学模型。</p><p>&emsp;&emsp;第一步是词嵌入。采用 100 维的 Glove 词向量（<a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>），将每个单词及标点都换成相应的词向量，那么一个句子就是若干个 100 维向量组成的序列。</p><p>&emsp;&emsp;第二步是序列标注。怎样使得模型的输出能够表征句子的关键词？序列标注是一种方法。参考中文分词的序列标注法，给每个字标上 B,M,E,S 中的一个标记，B 表示分词开始，M 表示词语中间字，E 表示分词结束，S 表示单字词语。这个序列唯一对应了一种分词结果。<br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn1.jpg"><br>&emsp;&emsp;同理应用到关键词提取任务中，可以想到一种标注方法：N 表示非关键词，B 表示关键词开始，M 表示关键词中间词，E 表示关键词结束，S 表示单个关键词。这样就使得关键词提取变成了分类任务，输入一个句子，为每一个单词做分类。</p><p>&emsp;&emsp;以“NBMES”来标注虽然十分准确，但是标注较为复杂，数据集较小时训练效果不好。<br>&emsp;&emsp;一种改进是改为“NBM”标注，N 表示非关键词，B 表示关键词开始，M 表示关键词非开始位置。这与“NBMES”标注是等价的（二者是唯一对应的），但是化简了标注。<br>&emsp;&emsp;另一种改进是改为“IO”标注，I 表示是关键词，O 表示不是关键词。这种标注法比上述方法更简便，且变成了二分类问题，可以使用更多评测指标（例如 F1 分数）。但缺点是无法区分关键词组与连续单个关键词。<br>&emsp;&emsp;本次任务采用“IO”标注。检视本次数据集发现所有关键词（组）均不相邻，因此上述缺点在训练中不存在。在实际应用中，也并非一定要区分关键词组与连续单个关键词。</p><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>&emsp;&emsp;句子的长度是变化的，不适合使用固定大小的全连接层。循环神经网络（RNN）可以有效应对这类数据。<br>&emsp;&emsp;（此处略去若干 RNN 原理。。。）<br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn2.png"></p><p>&emsp;&emsp;RNN 可以做许多扩展，例如：</p><ol><li>多个 RNN 垂直叠加，成为多层 RNN；</li><li>再增加一层从右影响到左的隐状态，成为双向 RNN；</li></ol><p>&emsp;&emsp;RNN 虽然理论上实现了当前状态与过往状态的联系，但对于时间相隔较长的过往状态保留的信息很少。同时,RNN 展开较深的时序时，存在梯度消失和梯度爆炸的问题。<br>&emsp;&emsp;LSTM 是一种改进的 RNN，在其结构中加入了许多控制门：<br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/lstm1.png"><br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/lstm2.png" title="来自pytorch官方文档"><br>&emsp;&emsp;LSTM 新增了一层隐状态 $\mathbf C_t$。传统 RNN 通过 $\mathbf x_t$ 与 $\mathbf h_{t-1}$ 直接得到 $\mathbf h_t$；但在 LSTM 中，先通过 $\mathbf x_t$ 与 $\mathbf h_{t-1}$ 使用不同的权重和偏置分别算出遗忘门 $\mathbf f_t$、输入门 $\mathbf i_t$、输出门 $\mathbf o_t$，再使用遗忘门和输入门对 $\mathbf C_t$ 进行更新，最后用 $\mathbf C_t$ 和输出门得到 $\mathbf h_t$。<br>&emsp;&emsp;三个控制门矩阵都经过了 sigmoid 函数，因此元素都 $\in (0,1)$，用这些矩阵对别的矩阵做对应位置相乘求和，相当于控制别的矩阵的每个元素的保留程度。因此 $\mathbf C_t = \mathbf f_t \odot \mathbf C_{t-1} + \mathbf i_t \odot \mathbf g_t$ 这一项，就相当于控制 $\mathbf C_{t-1}$ 遗忘了多少、当前 $\mathbf g_t$ 记住了多少，是一种平滑移动。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>&emsp;&emsp;整份代码实现起来难度不大，主要难度在于学习如何使用 pytorch。就只放网络结构了，其实也都是调包侠。。。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.utils.rnn <span class="keyword">as</span> tRnn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vec_dim, hidden_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        self.lstm = nn.LSTM(vec_dim, hidden_dim, batch_first = <span class="literal">True</span>, dropout = <span class="number">0.2</span>, bidirectional = <span class="literal">False</span>)</span><br><span class="line">        self.fc1 = nn.Linear( hidden_dim, <span class="number">64</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">64</span>, out_dim)</span><br><span class="line">        self.dropout10 = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x, _ = self.lstm(x)</span><br><span class="line">        x, _ = tRnn.pad_packed_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        x = self.fc1(F.relu(x))</span><br><span class="line">        x = self.fc2(F.relu(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br>&emsp;&emsp;还有数据的预处理也是比较繁琐的，但是也只是编程实现上的难度，不是算法设计的难度。<p></p><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>&emsp;&emsp;因为序列标注用了二分类，因此结果的评价就用 $F1$ 分数了</p><script type="math/tex;mode=display">F1 = \frac{2 \cdot \text{precision rate} \cdot \text{recall rate}}{\text{precision rate} + \text{recall rate}}</script><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn3.png"><h1 id="BiLSTM-CRF-中文分词"><a href="#BiLSTM-CRF-中文分词" class="headerlink" title="BiLSTM+CRF 中文分词"></a>BiLSTM+CRF 中文分词</h1><h2 id="Task-1"><a href="#Task-1" class="headerlink" title="Task"></a>Task</h2><p>&emsp;&emsp;使用 BiLSTM+CRF 分词模型，在 SIGHAN Microsoft Research 数据集上进行中文分词的训练和测试。<br>&emsp;&emsp;已标注数据集的每一条数据是一个中文句子，词之间用两个空格隔开。</p><h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2><p>&emsp;&emsp;依然是采用序列标注的方法来做分词，为每个字标上 B,M,E,S 中的一个标记，B 表示分词开始，M 表示词语中间字，E 表示分词结束，S 表示单字词语。这个序列唯一对应了一种分词结果。</p><p>&emsp;&emsp;模型的整体框架为：对于一个句子，设长度为 $l$（以 UTF8 字符数量计），依次通过以下步骤：</p><ol><li>将每个 UTF8 字符替换成字向量，得到长度为 $l$ 的字向量序列；</li><li>通过 BiLSTM 得到长度为 $l$ 的分数向量序列（分数向量是一个 4 维向量，分别代表该字表为 B(begin),M,E,S 的分数）；</li><li>通过 CRF 得到长度为 $l$ 的序列标注；</li><li>基于序列标注得到分词结果。</li></ol><p>&emsp;&emsp;采用 300 维的预训练字向量 sgns.context.word-character.char1-1.dynwin5.thr10.neg5.dim300.iter5，该向量集含有单字、词语、标点符号、英语缩写、希腊字母等诸多元素，但只取单字和标点符号用于本次任务。<br>&emsp;&emsp;本次任务中，若句子含有未被字向量覆盖的字符，则该数据作废。经验证，训练集有 1 条数据被作废，测试集数据全部有效。</p><h2 id="BiLSTM-CRF"><a href="#BiLSTM-CRF" class="headerlink" title="BiLSTM+CRF"></a>BiLSTM+CRF</h2><p>&emsp;&emsp;LSTM 上面已经写过了，BiLSTM 就是双向的 LSTM。<br>&emsp;&emsp;LSTM 的输出是 4 维分数向量的序列，进行归一化可以直接得到每个字的标签预测概率分布。但是结合传统概率模型可以使上下文关联效果更好，因此 LSTM 之后采用 CRF。<br>&emsp;&emsp;CRF 有两个任务：训练的时候求出给定序列标注的后验概率，并以其负对数作为损失函数进行梯度下降；测试的时候通过输入的分数向量序列求出概率最大的序列标注。<br>&emsp;&emsp;但实际上许多现有的 CRF 实现方法做的并不是真正的概率模型，而是用一种标注方案的分数除以所有标注方案分数和作为该方案的“概率”。以下先按概率模型作分析，再说明现有 CRF 实现方法的分数模型。</p><h3 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h3><p>&emsp;&emsp;训练时，设给定序列标注为 $y_1,\cdots,y_l$，CRF 的输入序列为 $\mathbf x_1,\cdots,\mathbf x_l$，则后验概率为</p><script type="math/tex;mode=display">P(y_1,\cdots,y_l\ |\ \mathbf x_1,\cdots,\mathbf x_l) = \left(\prod_{i=1}^l P(y_i|\mathbf x_i)\right)\left(\prod_{i=2}^{l}P(y_i|y_{i-1})\right)</script><p>&emsp;&emsp;求解该式只需将 $i$ 从 $1$ 到 $l$ 遍历一遍，把所有用到的概率乘起来即可。其中 $P(y_i|\mathbf x_i)$ 为发射概率，可以将向量 $\mathbf x_i$ 归一化之后取 $y_i$ 那一项；$P(y_i|y_{i-1})$ 为转移概率，可以用矩阵 $\mathbf A$ 来求，$A_{ij}$ 表示由标签 $i$ 转移到标签 $j$ 的概率，该矩阵与其他网络参数一起参与训练。<br>&emsp;&emsp;测试时，已有输入序列 $\mathbf x_1,\cdots,\mathbf x_l$ 和标签转移矩阵 $\mathbf A$，则通过动态规划找到最大概率的标注方案。设 $dp_{i,j}$ 表示已经考虑了序列的前 $i$ 项，第 $i$ 项选择标签 $j$，的最大概率，则</p><script type="math/tex;mode=display">dp_{i,j} = P(j|\mathbf x_i) \cdot \max_{k=1}^4 (dp_{i-1,k} \cdot A_{kj})</script><p>&emsp;&emsp;记录 DP 的转移路径，最后选最大的 $dp_{l,j}(j=1,2,3,4)$ 根据转移路径倒推即得到标注结果。<br>&emsp;&emsp;实现的时候应当对概率取对数，将乘法变为加法运算，避免精度问题。</p><h3 id="分数模型"><a href="#分数模型" class="headerlink" title="分数模型"></a>分数模型</h3><p>&emsp;&emsp;以 torchcrf 的实现为例：所有的概率都被换成分数，后验概率改为后验分数；发射概率 $P(y_i|\mathbf x_i)$ 替换为发射分数，直接取向量 $\mathbf x_i$ 中 $y_i$ 那一维的值；转移概率矩阵 $\mathbf A$ 替换为转移分数矩阵，$A_{ij}$ 表示由标签 $i$ 转移到标签 $j$ 可以获得的分数。上面两式的概率乘法改为分数相加。</p><script type="math/tex;mode=display">\begin{aligned}
& F(y_1,\cdots,y_l\ |\ \mathbf x_1,\cdots,\mathbf x_l) = \left(\sum_{i=1}^l F(y_i|\mathbf x_i)\right)+\left(\sum_{i=2}^{l}F(y_i|y_{i-1})\right) \\ 
& dp_{i,j} = F(j|\mathbf x_i) + \max_{k=1}^4 (dp_{i-1,k} + A_{kj})
\end{aligned}</script><p>&emsp;&emsp;训练时还需计算一个“所有标注方案的分数和”，可以用类似递推算出。<br>&emsp;&emsp;计算损失函数的时候，定义给定标注序列 $y_1,\cdots,y_l$ 的“概率”为</p><script type="math/tex;mode=display">\frac{exp(\text{序列} y_1,\cdots,y_l \text{的分数})}{exp(\text{所有标注方案的分数和})}</script><p>&emsp;&emsp;取对数则为</p><script type="math/tex;mode=display">(\text{序列} y_1,\cdots,y_l \text{的分数}) - (\text{所有标注方案的分数和})</script><h2 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h2><p>&emsp;&emsp;依然是大调包，只放网络结构了。<br>&emsp;&emsp;网络里的结构依次为：BiLSTM，全连接层，CRF。在训练的前向过程中，CRF 层是计算“概率”的对数，得到结果取反为损失函数；而测试的前向过程，则是动态规划解码，直接返回标注序列。<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.utils.rnn <span class="keyword">as</span> tRnn</span><br><span class="line"><span class="keyword">from</span> torchcrf <span class="keyword">import</span> CRF</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BiLSTM_CRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vec_dim, hidden_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLSTM_CRF, self).__init__()</span><br><span class="line">        self.lstm = nn.LSTM(vec_dim, hidden_dim//<span class="number">2</span>, batch_first = <span class="literal">True</span>, dropout = <span class="number">0.2</span>, bidirectional = <span class="literal">True</span>)</span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, out_dim)</span><br><span class="line">        self.dropout10 = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.crf = CRF(out_dim, batch_first = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_train</span>(<span class="params">self, x, label, mask</span>):</span><br><span class="line">        x, _ = self.lstm(x)</span><br><span class="line">        x, _ = tRnn.pad_packed_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># x = self.dropout10(x)</span></span><br><span class="line">        x = self.hidden2tag(x)</span><br><span class="line">        x = -self.crf(x, label, mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_test</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        x, _ = self.lstm(x)</span><br><span class="line">        x, _ = tRnn.pad_packed_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x = self.hidden2tag(x)</span><br><span class="line">        x = self.crf.decode(x, mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p></p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>&emsp;&emsp;评估指标为 F1：</p><script type="math/tex;mode=display">\begin{aligned}
F1 &= \frac{2 \times precision \times recall}{precision + recall} \\ 
precision &= \text{分对词数}/\text{预测分词结果的总词数} \\ 
recall &= \text{分对词数}/\text{标准分词结果的总词数}
\end{aligned}</script><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn4.png"><h1 id="LSTM-语言模型"><a href="#LSTM-语言模型" class="headerlink" title="LSTM 语言模型"></a>LSTM 语言模型</h1><h2 id="Task-2"><a href="#Task-2" class="headerlink" title="Task"></a>Task</h2><p>&emsp;&emsp;语言模型是机器对于语言本身的掌握程度，形式化地定义就是，给定一个句子，计算这个句子的概率。<br>&emsp;&emsp;这东西的应用，比如，翻译标准“信达雅”中的“达”，即目标语言的通顺流畅，就是语言模型；比如，输入法或者百度搜索框里面输入了一个词，系统就会联想下一个词，或者联想了一个句子，这个是文本生成。<br>&emsp;&emsp;本次任务就是文本生成。给定初始词，用这个词生成下一个词，再用新生成的词生成下一个词，再用新生成的词生成下一个词……直至生成\<eof \>表示句子的结束。用 LSTM 实现。训练集仍是 SIGHAN Microsoft Research。</eof></p><h2 id="原理与步骤"><a href="#原理与步骤" class="headerlink" title="原理与步骤"></a>原理与步骤</h2><p>&emsp;&emsp;模型核心为一个单向 LSTM，输入是一个句子转化成的词向量序列，训练时通过 LSTM 得到相同长度的预测概率分布序列（序列的每一个元素是词表大小的向量，表示预测为词表某个词的概率分布），以与正确文本的逐词交叉熵作为损失函数进行反向传播；测试时给定初始词，不断用上一个词通过 LSTM 生成下一个词，直至生成 \<eos \>句末标志。</eos></p><h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><p>&emsp;&emsp;词表构建有两种方法，一是沿用上一个 Task 的 300 维预训练字词向量 sgns.context.word-character.char1-1.dynwin5.thr10.neg5.dim300.iter5，它包含了绝大部分常用词语。但该方法有很多缺点：</p><ol><li>词表有 63 万个词，搭建网络对显存需求很大；</li><li>训练集被词表完全覆盖的句子只有 39231 条，意味着超过一半的句子存在“未知词”；</li><li>观察词表发现 150000 以后的词并不常用（如各种网络用语），几乎不会出现在训练集里，造成浪费。</li></ol><p>&emsp;&emsp;第二种方法是仅给训练集里出现的词语进行编号，用 pytorch 的 embedding 层得到词向量，随其他网络参数一同训练。训练集的词语约 88000 条，相比上一个方法能有效减小网络大小，且完全覆盖训练集。</p><p>&emsp;&emsp;本次两种方法都尝试了一遍，一是截取预训练词表前 150000 个词，且训练集只保留被词向量完全覆盖的句子（否则预测出来的句子也会大量出现“未知词”）；二是给训练集的词编号并使用 embedding 层，词向量跟随网络一起训练。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>&emsp;&emsp;网络里的结构依次为：(embedding)，LSTM，全连接层。词表大小为 <code>vocab\_size</code>，embedding 层维度为 <code>vocab\_size$$\to$$embedding\_dim</code>，LSTM 层维度为 <code>embedding\_dim$$\to$$hidden\_dim</code>，全连接层维度 <code>hidden\_dim$$\to$$vocab\_size</code>。<br>&emsp;&emsp;若采用预训练词向量则没有 embedding 层。<br>&emsp;&emsp;由于显存的限制，<code>hidden\_dim</code> 只能开到 350，造成该网络是比较畸形的网络。（后面全连接层输出至少是 8 万维）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.utils.rnn <span class="keyword">as</span> tRnn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim, out_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first = <span class="literal">True</span>, dropout = <span class="number">0.2</span>)</span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, out_dim)</span><br><span class="line">        self.dropout10 = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_train</span>(<span class="params">self, x, data_length</span>):</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = tRnn.pack_padded_sequence(x, data_length, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x, _ = self.lstm(x)</span><br><span class="line">        x, _ = tRnn.pad_packed_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x = self.dropout10(x)</span><br><span class="line">        x = self.hidden2tag(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_test</span>(<span class="params">self, x, data_length</span>):</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = tRnn.pack_padded_sequence(x, data_length, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x, _ = self.lstm(x)</span><br><span class="line">        x, _ = tRnn.pad_packed_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line">        x = self.hidden2tag(x)</span><br><span class="line">        _, x = torch.<span class="built_in">max</span>(x,<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>&emsp;&emsp;由于 pytorch 比较难实现直接通过上一个词得出下一个词，故用以下方式实现：<br>&emsp;&emsp;设当前句子为 $sentence$（是一个词序列），初始时 $sentence$ 为输入的初始词。每次将 $sentence$ 输入网络，得到相同长度的输出 $predict$，取 $predict$ 的最后一个词添加到 $sentence$ 的末尾，若该词不为 \<eos \>则重复该过程。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">initWord, myNetwork</span>) :</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span>(initWord <span class="keyword">in</span> wordVec) :</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;This word is not in training set!&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    sentence = [wordVec[initWord]]</span><br><span class="line">    myNetwork.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        data = torch.tensor([sentence])</span><br><span class="line">        predict = myNetwork.forward_test(data,[<span class="built_in">len</span>(sentence)])[<span class="number">0</span>]</span><br><span class="line">        lastWord = predict[-<span class="number">1</span>]</span><br><span class="line">        sentence.append(lastWord)</span><br><span class="line">        <span class="keyword">if</span> (lastWord==<span class="number">0</span>) :</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sentence :</span><br><span class="line">        <span class="built_in">print</span>(vecWord[i],end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure></eos></p><h2 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h2><p>&emsp;&emsp;迭代 30 次，取每次 epoch 的所有句子的平均交叉熵，对比如下：<br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn5.png"></p><p>&emsp;&emsp;可以看到，使用训练集作为词表的效果会略微好些，差距大约在 15 次 epoch 时被拉开。</p><p>&emsp;&emsp;生成句子的测试效果如下：<br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn6.png" title="g1"><br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn7.png" title="g2"><br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn8.png" title="g3"><br><img src="/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/rnn9.png" title="g4"><br>&emsp;&emsp;4 个图分别称作图 1 ~ 图 4，分别为预训练词向量测试效果1、预训练词向量测试效果2、训练集embedding 测试效果1、训练集embedding 测试效果2。<br>&emsp;&emsp;整体来看，二者都能正常执行生成句子的过程，生成的句子大致通顺，但存在大量语法问题，句子成分缺失明显，句意基本都不通顺。<br>&emsp;&emsp;相比来看，在测试集 1 上训练集 embedding 的效果（图 3）要好于预训练词向量的效果（图 1），后者出现了大量的重复词语和句子，且句子通顺程度弱于前者；测试集 2 虽然整体来看是训练集 embedding 更通顺（图 4），但预训练词向量的结果（图 2）中有“给北京军区某给水工程团记一等功。”这样完全符合语法、句意完全通顺的句子。<br>&emsp;&emsp;以图 3 为代表，可以发现生成的句子具有如下特点：</p><ol><li>有关党和政治的语句特别多，与党和政治相关的开头（如图 3 的前三句）效果要好于其他句子，甚至用“诗”作开头都能扯到党的建设。这是明显的训练集特征，训练集大部分句子是与党和政治相关的。</li><li>部分句子会使用训练集原文。如图3 的“美国”开头的句子，这条新闻可以在百度搜到原文。</li><li>稍有典雅性。如图 3 的后三句——“墨色生几分侵蚀”“诗才敏捷，音韵铿锵”等，尽管这完全不足以进行文学分析。</li></ol></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="http://kqp.world/%E3%80%90%E8%8B%A5%E5%B9%B2%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91RNN%E4%B8%89%E8%BF%9E/" title="【若干大作业】RNN三连" target="_blank" rel="external">http://kqp.world/【若干大作业】RNN三连/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！</li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.png" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">kqp</span><small class="ml-1x">OIACMer / HKU_CS / LLer / TCS</small></a></h3><div>糖少许，盐少许</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/%E3%80%902020%20%E9%BB%91%E9%BE%99%E6%B1%9F%E7%9C%81%E8%B5%9B%20E%E3%80%91Everybody%20Lost%20Somebody%20%E9%A2%98%E8%A7%A3/" title="【2020 黑龙江省赛 E】Everybody Lost Somebody 题解"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a></li><li class="next"><a href="/%E3%80%902020%20Multi-University%204%20I%E3%80%91Imperative%20Meeting%20%E9%A2%98%E8%A7%A3/" title="【2020 Multi-University 4 I】Imperative Meeting 题解"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li></ul><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="mailto:kuangqp1217@qq.com" target="_blank" title="Email" data-toggle="tooltip" data-placement="top"><i class="icon icon-email"></i></a></li><li><a href="http://wpa.qq.com/msgrd?v=3&uin=940240973&site=qq&menu=yes" target="_blank" title="Qq" data-toggle="tooltip" data-placement="top"><i class="icon icon-qq"></i></a></li><li><a href="https://twitter.com/yuan_four" target="_blank" title="Twitter" data-toggle="tooltip" data-placement="top"><i class="icon icon-twitter"></i></a></li><li><a href="https://github.com/l2l7l9p" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta=(meta="nick,mail,link").split(",").filter(function(e){return-1<GUEST.indexOf(e)});new Valine({el:"#vcomments",verify:!1,notify:!0,appId:"YsBbwGr5CQ2WyuobqGxOedtu-gzGzoHsz",appKey:"Wplm0hz1L65wFVye5hE8tOTf",placeholder:"Just go go",avatar:"mm",meta:meta,pageSize:"10",visitor:!1})</script></body></html>